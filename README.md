# MLOps Project: YouTube Comments Topic Modeling

This project implements an automated Machine Learning Operations (MLOps) pipeline designed to collect YouTube comments on a specific topic, analyze them, identify underlying themes (topic modeling), and predict these themes for new data. The entire process is orchestrated using **Apache Airflow** and utilizes **MLflow** for model tracking and versioning.

## Project Description

The primary goal of this project is to extract unstructured text data (YouTube comments), transform it into a structured format, and apply machine learning methods to automatically classify the content by topic.

Key features include:

*   **Automation**: Airflow DAGs run the training and prediction processes on a predefined schedule.
*   **Data Collection**: Integration with the YouTube API to fetch fresh comments based on a keyword (`psychology`).
*   **Text Processing**: Robust cleaning and lemmatization tailored for the Russian language.
*   **Modeling**: Application of **spectral clustering** to generate topic labels (as the data is unlabelled) and subsequent training of a **logistic regression** classifier to predict these topics.
*   **MLOps**: Utilizing **MLflow** for logging parameters, metrics, and versioning key models (TF-IDF vectorizer and classifier).
*   **Configuration**: All parameters are centrally managed via a single `params_all.yaml` file.

## Project Structure

The standard hierarchical structure ensures modularity and clean code separation.

```text
‚îú‚îÄ‚îÄ config/
‚îÇ ‚îî‚îÄ‚îÄ params_all.yaml         # Configuration file for train/predict stages
‚îú‚îÄ‚îÄ dags/
‚îÇ ‚îî‚îÄ‚îÄ test_train_dag.py       # Apache Airflow DAG definitions
‚îú‚îÄ‚îÄ data/
‚îÇ ‚îî‚îÄ‚îÄ text.csv                # Output file with top words per topic
‚îú‚îÄ‚îÄ notebooks/
‚îÇ ‚îî‚îÄ‚îÄ Topic modeling.ipynb    # Jupyter notebook for EDA/prototyping
‚îú‚îÄ‚îÄ src/
‚îÇ ‚îú‚îÄ‚îÄ cluster_train.py        # Clustering logic and metric calculations
‚îÇ ‚îú‚îÄ‚îÄ get_comments.py         # YouTube API data collection module
‚îÇ ‚îî‚îÄ‚îÄ preprocessing_text.py   # Text cleaning and lemmatization functions
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ predict.py                # Inference (prediction) script
‚îî‚îÄ‚îÄ train.py                  # Model training script
```

## Technologies Used

| Category | Tool/Library | Purpose |
| :--- | :--- | :--- |
| **Orchestration** | `Apache Airflow` | Scheduling and running tasks (`train`, `predict`). |
| **MLOps** | `MLflow` | Experiment tracking, model registration, and versioning. |
| **Data Handling** | `Python`, `pandas`, `numpy` | Core data manipulation logic. |
| **NLP** | `nltk`, `pymystem3` (`Mystem`) | Stopword removal, Russian language lemmatization. |
| **Modeling** | `scikit-learn` | TF-IDF, Spectral Clustering, Logistic Regression. |
| **API** | `requests`, `pyyoutube` | Interaction with the YouTube Data API v3. |
| **Configuration** | `PyYAML` | Managing project parameters. |

## Pipeline Overview (Workflow)

1.  **Orchestration (`dags/test_train_dag.py`)**: Airflow triggers `train.py` at 01:00 AM hourly and `predict.py` at 03:00 AM hourly.
2.  **Data Collection (`src/get_comments.py`)**: Both scripts fetch fresh comments from YouTube using the query "–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è".
3.  **Preprocessing (`src/preprocessing_text.py`)**: Text is cleaned of emojis/links and lemmatized using Mystem.
4.  **Training (`train.py`, `src/cluster_train.py`)**:
    *   Data is vectorized (TF-IDF).
    *   Optimal topic count is automatically determined via spectral clustering and silhouette score evaluation.
    *   A classifier (`LogisticRegression`) is trained to predict these discovered topics.
    *   Models and metrics are logged to MLflow.
    *   Model version numbers are updated in `params_all.yaml`.
5.  **Prediction (`predict.py`)**:
    *   The latest model versions are loaded from MLflow.
    *   New comments are classified.
    *   The top 10 keywords for each identified topic are saved to `data/text.csv`.

## Results Interpretation: Identified Topics

The script processed YouTube comments (using "–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è" as the query) and identified 9 distinct topics. The following table presents the top 10 keywords characterizing each theme, providing insight into the community's primary areas of interest. The topics are organized in columns as they appear in the output CSV file.

| Topic ID | Top Keywords | Interpretation |
| :--- | :--- | :--- |
| **0** | —á–µ–ª–æ–≤–µ–∫, –≤–ª–∞—Å—Ç—å, —Å–∞–º—ã–π, –≥–Ω–∏–ª–æ–π, –¥–µ–ª–∞—Ç—å, –∂–∏–∑–Ω—å, –ª—é–±–æ–≤—å, –ø–æ–Ω–∏–º–∞—Ç—å, –ª—é–±–∏—Ç—å, –±–æ—è—Ç—å—Å—è | **Core Human Concepts & Challenges**. This topic groups general philosophical and human existence terms ("human", "life", "love") mixed with concepts of power, criticism ("rotten"), and fear. It seems to represent a high-level, foundational theme about the human experience. **–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è: –∂–∏–∑–Ω—å, –ª—é–±–æ–≤—å, –≤–ª–∞—Å—Ç—å –∏ —Å—Ç—Ä–∞—Ö–∏.** |
| **1** | –≤–∏–¥–µ–æ, –ø—Å–∏—Ö–æ–ª–æ–≥, –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è, –∫–∞–Ω–∞–ª, –ª–µ–∫—Ü–∏—è, –∞–≤—Ç–æ—Ä, –≥–ª–∞–∑, —Ç–µ–º–∞, –ø–æ–Ω–∏–º–∞—Ç—å, –¥–∞–≤–Ω–æ | **Content Consumption & Expert Analysis**. This theme focuses heavily on how users consume the content: videos, lectures, podcasts, and the role of the psychologist/author. It represents the active engagement with the media itself. **–û–±—Å—É–∂–¥–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–≤–∏–¥–µ–æ, –ª–µ–∫—Ü–∏–∏) –∏ —Ä–æ–ª–∏ –∞–≤—Ç–æ—Ä–∞/–ø—Å–∏—Ö–æ–ª–æ–≥–∞.** |
| **2** | –∑–∞–±–æ—Ç–∏—Ç—å—Å—è, –ª–µ–∫—Ü–∏—è, –ª–∏—Ü–æ, –ª—é–±–∏—Ç—å, –ª—é–±–æ–≤—å, –ª—é–±–æ–π, –º–∞–ª–æ, –º–∞–ª—å—á–∏–∫, –º–∞–º–∞, –∏–¥–µ—è | **Emotional & Relational Dynamics**. This topic touches upon personal emotions and relationships, particularly family ("mama", "boy", "care", "love"). It suggests discussions around emotional well-being and personal connections. **–õ–∏—á–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è, —ç–º–æ—Ü–∏–∏ –∏ —Å–µ–º–µ–π–Ω—ã–µ —Ç–µ–º—ã, –≤–∫–ª—é—á–∞—è –∑–∞–±–æ—Ç—É –∏ –ª—é–±–æ–≤—å.** |
| **3** | –±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å, —Ä–æ–ª–∏–∫, –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π, –¥–µ–Ω—å–≥–∏, —É–≤–∏–¥–µ—Ç—å, —Ä–∞–±–æ—Ç–∞, –¥—É—à–∞, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –≤–∏–¥–µ–æ, —Å–ª–æ–≤–æ | **Value Exchange & Information Utility**. This theme mixes expressions of gratitude ("thank") with the perceived value of the information ("useful information"), discussions about work, and potentially the financial aspects ("money") of expert advice. **–ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å –∑–∞ –ø–æ–ª–µ–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –æ–±—Å—É–∂–¥–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.** |
| **4** | –≥–æ–¥, –±–∞–±—É—à–∫–∞, –ª—é–±–∏—Ç—å, —Ä–µ–±–µ–Ω–æ–∫, –º–∞–º–∞, –∂–∏–∑–Ω—å, –¥–µ—Ç—Å—Ç–≤–æ, –ø–æ–Ω–∏–º–∞—Ç—å, —Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è, —Å—É–ø | **Generational & Life Stages**. This topic clearly groups terms related to time, age, family, and life stages ("grandmother", "child", "mother", "childhood", "year"). It likely covers content related to family dynamics and growing up. **–¢–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –≤–æ–∑—Ä–∞—Å—Ç–æ–º, —Å–µ–º—å–µ–π, –¥–µ—Ç—Å—Ç–≤–æ–º –∏ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º–∏ —ç—Ç–∞–ø–∞–º–∏.** |
| **5** | —Å–ø–∞—Å–∏–±–æ, –±–æ–ª—å—à–æ–π, –ø–æ–¥–∫–∞—Å—Ç, –æ–≥—Ä–æ–º–Ω—ã–π, –≤–∏–¥–µ–æ, –∂–∏–∑–Ω—å, –≥–æ—Å—Ç—å, –ø–æ–ª–µ–∑–Ω—ã–π, —Ç–µ–º–∞, —Å–æ–≤–µ—Ç | **Feedback, Format, & Utility**. A strong theme of positive feedback ("thank you", "great") combined with specific content formats ("podcast", "guest") and the practical utility of the advice given ("useful", "advice"). **–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã, –æ–±—Å—É–∂–¥–µ–Ω–∏–µ –ø–æ–¥–∫–∞—Å—Ç–æ–≤/–≥–æ—Å—Ç–µ–π –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–ª—å–∑—ã —Å–æ–≤–µ—Ç–æ–≤.** |
| **6** | —Å–ª—É—à–∞—Ç—å, –∫–Ω–∏–≥–∞, –ø—Ä—è–º–æ–π, —á–∏—Ç–∞—Ç—å, –ø—Ä–∏—è—Ç–Ω–æ, –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è, –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, –±—Ä–∞—Ç—å, –º–∏–Ω—É—Ç–∞, –Ω–∞—á–∏–Ω–∞—Ç—å | **Learning Process & Engagement**. This topic focuses on the active steps users take to learn: listening, reading books, "starting" a process, and finding content interesting or pleasant. **–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ/—á—Ç–µ–Ω–∏–µ, –Ω–∞—á–∞–ª–æ –Ω–æ–≤—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫.** |
| **7** | –∂–µ–Ω—â–∏–Ω–∞, –º—É–∂–∏–∫, –º—É–∂—á–∏–Ω–∞, —Ä–µ–±–µ–Ω–æ–∫, –æ—Å–æ–±–µ–Ω–Ω–æ, –≥–æ—Ç–æ–≤–∏—Ç—å, –¥–µ–ª–∞—Ç—å, –¥–µ–Ω—å–≥–∏, –¥–∞–ª–µ–∫–æ, –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è | **Gender Roles, Action, & Criticism**. This theme seems to be more action-oriented ("do", "prepare") but also includes specific gender references ("woman", "man") and potentially darker or critical concepts like "money" and "manipulation". **–û–±—Å—É–∂–¥–µ–Ω–∏–µ –≥–µ–Ω–¥–µ—Ä–Ω—ã—Ö —Ä–æ–ª–µ–π, –¥–µ–π—Å—Ç–≤–∏–π –∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–º–µ—á–∞–Ω–∏—è (–¥–µ–Ω—å–≥–∏, –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏).** |
| **8** | –±–æ–≥, –º–∏—Ä, —Ç–≤–æ–π, —Å–∞–º—ã–π, –ø—Ä–∞–≤–¥–∞, —Å–ª–æ–≤–æ, –ª—é–±–æ–≤—å, —Ä–µ—à–∞—Ç—å, –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –≤–∑—è—Ç—å | **Existential & Conclusive Concepts**. This final topic is highly abstract and conclusive: discussing "God", the "world", "truth", "love", and "solving" issues. It suggests a focus on existential themes or summarizing outcomes. **–≠–∫–∑–∏—Å—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã: –ø–æ–∏—Å–∫ –∏—Å—Ç–∏–Ω—ã, –º–∏—Ä–∞, –ª—é–±–≤–∏ –∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º.** |


## üß† Deep Dive: ML Logic & Mathematical Base
The data processing workflow is divided into three key stages: text-to-numerical transformation, discovery of latent structures (clustering), and predictive model training.

**1. Vectorization: TF-IDF Transformation**

To enable algorithms to process text, we convert it into a vector space using TF-IDF (Term Frequency-Inverse Document Frequency).
* **Term Frequency (TF)**: Evaluates the importance of a word within a single comment.
* **Inverse Document Frequency (IDF)**: Reduces the weight of words that appear frequently across all comments (e.g., common stop words), highlighting terms specific to particular topics.

**Mathematical Base**:The weight of term $t$ in document $d$ is calculated as:

$$w_{t,d} = \text{tf}_{t,d} \times \log\left(\frac{N}{\text{df}_t}\right)$$
  
where $N$ is the total number of comments, and $\text{df}_t$ is the number of documents containing term $t$. This results in a sparse matrix where each comment is represented by a vector of significant features.
  
**2. Topic Discovery: Spectral Clustering & Silhouette Score**

Unlike traditional K-Means, **Spectral Clustering** utilizes the eigenvalues of similarity matrices to perform dimensionality reduction before clustering.

* **Why Spectral Clustering?** It excels at handling data where cluster boundaries have complex shapes, which is typical for natural language semantics.

* **Automated Topic Detection**: The number of topics is not set manually. Instead, the script iterates through a range of values and evaluates each using the **Silhouette Score**.

**Mathematical Base (Silhouette Score)**: For each object, the coefficient is calculated as:

$$s = \frac{b - a}{\max(a, b)}$$

Where:
* $a$ is the mean distance to objects within the same cluster.
* $b$ is the mean distance to objects in the nearest neighboring cluster.The value $s$ ranges from $[-1, 1]$. The script selects the number of clusters that maximizes the mean silhouette score across the entire dataset.

**3. Classification: Logistic Regression**

Once clustering has assigned a topic ID (Label) to each comment, we train a **Logistic Regression** classifier to predict these labels.
* **Why this step?** Clustering is computationally expensive. A trained classification model (Inference) operates almost instantaneously, allowing for the classification of new comments without recalculating the similarity matrix for the entire dataset.
* **Logic**: The model constructs separating hyperplanes in the high-dimensional TF-IDF feature space.

**Mathematical Base**: For multi-class classification, the **Softmax** function is used to transform model outputs into probabilities for each topic:

$$P(y=i | x) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

where $z_i$ is the linear combination of features for class $i$. The model minimizes the Cross-Entropy loss function to ensure the highest possible prediction accuracy.

## üìà Metrics and Quality Control

During the training process, the following indicators are logged in **MLflow**:
* **Precision (Macro)**: Measures how accurately the model identifies a topic (minimizing false positives).
* **F1-Score (Macro)**: The harmonic mean of Precision and Recall, accounting for potential imbalances in the number of comments across different topics.
